{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be5f2327-655c-4596-a1ed-314263df878e",
   "metadata": {},
   "source": [
    "## Creating a RAG system to extract the content from the pdf and build a Q&A system [(link)](https://arxiv.org/pdf/2404.07143.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10a22850-dc14-42b2-bf81-84f853800b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pypdf\n",
      "  Downloading pypdf-4.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
      "Downloading pypdf-4.2.0-py3-none-any.whl (290 kB)\n",
      "   ---------------------------------------- 0.0/290.4 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 122.9/290.4 kB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 290.4/290.4 kB 3.6 MB/s eta 0:00:00\n",
      "Installing collected packages: pypdf\n",
      "Successfully installed pypdf-4.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cba8cfc9-9da5-48d9-9466-169f34b287fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain_community\n",
      "  Downloading langchain_community-0.0.34-py3-none-any.whl.metadata (8.5 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain_community) (6.0.1)\n",
      "Collecting SQLAlchemy<3,>=1.4 (from langchain_community)\n",
      "  Downloading SQLAlchemy-2.0.29-cp311-cp311-win_amd64.whl.metadata (9.8 kB)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain_community)\n",
      "  Downloading aiohttp-3.9.5-cp311-cp311-win_amd64.whl.metadata (7.7 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
      "  Downloading dataclasses_json-0.6.4-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting langchain-core<0.2.0,>=0.1.45 (from langchain_community)\n",
      "  Downloading langchain_core-0.1.45-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.0 (from langchain_community)\n",
      "  Downloading langsmith-0.1.50-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain_community) (1.25.2)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain_community) (2.31.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain_community) (8.2.3)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.1.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading frozenlist-1.4.1-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading multidict-6.0.5-cp311-cp311-win_amd64.whl.metadata (4.3 kB)\n",
      "Collecting yarl<2.0,>=1.0 (from aiohttp<4.0.0,>=3.8.3->langchain_community)\n",
      "  Downloading yarl-1.9.4-cp311-cp311-win_amd64.whl.metadata (32 kB)\n",
      "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading marshmallow-3.21.1-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.2.0,>=0.1.45->langchain_community)\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting packaging<24.0,>=23.2 (from langchain-core<0.2.0,>=0.1.45->langchain_community)\n",
      "  Downloading packaging-23.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pydantic<3,>=1 (from langchain-core<0.2.0,>=0.1.45->langchain_community)\n",
      "  Downloading pydantic-2.7.1-py3-none-any.whl.metadata (107 kB)\n",
      "     ---------------------------------------- 0.0/107.3 kB ? eta -:--:--\n",
      "     -------------------------------------- 107.3/107.3 kB 3.1 MB/s eta 0:00:00\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain_community)\n",
      "  Downloading orjson-3.10.1-cp311-none-win_amd64.whl.metadata (50 kB)\n",
      "     ---------------------------------------- 0.0/50.9 kB ? eta -:--:--\n",
      "     ---------------------------------------- 50.9/50.9 kB 1.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain_community) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain_community) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langchain_community) (2023.7.22)\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (4.11.0)\n",
      "Collecting greenlet!=0.4.17 (from SQLAlchemy<3,>=1.4->langchain_community)\n",
      "  Downloading greenlet-3.0.3-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.45->langchain_community) (2.4)\n",
      "Collecting annotated-types>=0.4.0 (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.45->langchain_community)\n",
      "  Downloading annotated_types-0.6.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pydantic-core==2.18.2 (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.45->langchain_community)\n",
      "  Downloading pydantic_core-2.18.2-cp311-none-win_amd64.whl.metadata (6.7 kB)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Downloading langchain_community-0.0.34-py3-none-any.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 0.4/1.9 MB 12.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.4/1.9 MB 17.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 20.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 15.4 MB/s eta 0:00:00\n",
      "Downloading aiohttp-3.9.5-cp311-cp311-win_amd64.whl (370 kB)\n",
      "   ---------------------------------------- 0.0/370.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 370.8/370.8 kB 11.3 MB/s eta 0:00:00\n",
      "Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
      "Downloading langchain_core-0.1.45-py3-none-any.whl (291 kB)\n",
      "   ---------------------------------------- 0.0/291.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 291.3/291.3 kB 6.0 MB/s eta 0:00:00\n",
      "Downloading langsmith-0.1.50-py3-none-any.whl (115 kB)\n",
      "   ---------------------------------------- 0.0/115.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 115.5/115.5 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading SQLAlchemy-2.0.29-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------- ----------------- 1.2/2.1 MB 37.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.1 MB 33.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  2.1/2.1 MB 33.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.1/2.1 MB 14.7 MB/s eta 0:00:00\n",
      "Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Downloading frozenlist-1.4.1-cp311-cp311-win_amd64.whl (50 kB)\n",
      "   ---------------------------------------- 0.0/50.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 50.5/50.5 kB 2.5 MB/s eta 0:00:00\n",
      "Downloading greenlet-3.0.3-cp311-cp311-win_amd64.whl (292 kB)\n",
      "   ---------------------------------------- 0.0/292.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 292.8/292.8 kB 9.1 MB/s eta 0:00:00\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
      "   ---------------------------------------- 0.0/49.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 49.4/49.4 kB 1.3 MB/s eta 0:00:00\n",
      "Downloading multidict-6.0.5-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Downloading orjson-3.10.1-cp311-none-win_amd64.whl (139 kB)\n",
      "   ---------------------------------------- 0.0/139.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 139.1/139.1 kB 4.2 MB/s eta 0:00:00\n",
      "Downloading packaging-23.2-py3-none-any.whl (53 kB)\n",
      "   ---------------------------------------- 0.0/53.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 53.0/53.0 kB 1.4 MB/s eta 0:00:00\n",
      "Downloading pydantic-2.7.1-py3-none-any.whl (409 kB)\n",
      "   ---------------------------------------- 0.0/409.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 409.3/409.3 kB 12.9 MB/s eta 0:00:00\n",
      "Downloading pydantic_core-2.18.2-cp311-none-win_amd64.whl (1.9 MB)\n",
      "   ---------------------------------------- 0.0/1.9 MB ? eta -:--:--\n",
      "   ----------------------- ---------------- 1.1/1.9 MB 34.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  1.9/1.9 MB 30.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.9/1.9 MB 20.3 MB/s eta 0:00:00\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.9.4-cp311-cp311-win_amd64.whl (76 kB)\n",
      "   ---------------------------------------- 0.0/76.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 76.7/76.7 kB 4.2 MB/s eta 0:00:00\n",
      "Downloading annotated_types-0.6.0-py3-none-any.whl (12 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Installing collected packages: pydantic-core, packaging, orjson, mypy-extensions, multidict, jsonpatch, greenlet, frozenlist, annotated-types, yarl, typing-inspect, SQLAlchemy, pydantic, marshmallow, aiosignal, langsmith, dataclasses-json, aiohttp, langchain-core, langchain_community\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "Successfully installed SQLAlchemy-2.0.29 aiohttp-3.9.5 aiosignal-1.3.1 annotated-types-0.6.0 dataclasses-json-0.6.4 frozenlist-1.4.1 greenlet-3.0.3 jsonpatch-1.33 langchain-core-0.1.45 langchain_community-0.0.34 langsmith-0.1.50 marshmallow-3.21.1 multidict-6.0.5 mypy-extensions-1.0.0 orjson-3.10.1 packaging-23.2 pydantic-2.7.1 pydantic-core-2.18.2 typing-inspect-0.9.0 yarl-1.9.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9341700d-4c4b-4a96-b42a-ce10ca578a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-text-splitters\n",
      "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: langchain-core<0.2.0,>=0.1.28 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-text-splitters) (0.1.45)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (6.0.1)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (1.33)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (0.1.50)\n",
      "Requirement already satisfied: packaging<24.0,>=23.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (23.2)\n",
      "Requirement already satisfied: pydantic<3,>=1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (2.7.1)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (8.2.3)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (3.10.1)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (2.31.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.18.2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (2.18.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic<3,>=1->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.0->langchain-core<0.2.0,>=0.1.28->langchain-text-splitters) (2023.7.22)\n",
      "Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: langchain-text-splitters\n",
      "Successfully installed langchain-text-splitters-0.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install -U langchain-text-splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6201aa72-d5d4-421e-9f39-f6fb56d5f53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n",
    "from IPython.display import Markdown as md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac9d5254-6998-41d1-a875-fce26146964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(\"https://arxiv.org/pdf/2404.07143.pdf\")\n",
    "pages = loader.load_and_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebc85395-78b2-48a0-a7dc-1e9fa96759a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Preprint. Under review.\\nLeave No Context Behind:\\nEfficient Infinite Context Transformers with Infini-attention\\nTsendsuren Munkhdalai, Manaal Faruqui and Siddharth Gopal\\nGoogle\\ntsendsuren@google.com\\nAbstract\\nThis work introduces an efficient method to scale Transformer-based Large\\nLanguage Models (LLMs) to infinitely long inputs with bounded memory\\nand computation. A key component in our proposed approach is a new at-\\ntention technique dubbed Infini-attention. The Infini-attention incorporates\\na compressive memory into the vanilla attention mechanism and builds\\nin both masked local attention and long-term linear attention mechanisms\\nin a single Transformer block. We demonstrate the effectiveness of our\\napproach on long-context language modeling benchmarks, 1M sequence\\nlength passkey context block retrieval and 500K length book summarization\\ntasks with 1B and 8B LLMs. Our approach introduces minimal bounded\\nmemory parameters and enables fast streaming inference for LLMs.\\n1 Introduction\\nMemory serves as a cornerstone of intelligence, as it enables efficient computations tailored\\nto specific contexts. However, Transformers (Vaswani et al., 2017) and Transformer-based\\nLLMs (Brown et al., 2020; Touvron et al., 2023; Anil et al., 2023; Groeneveld et al., 2024) have\\na constrained context-dependent memory, due to the nature of the attention mechanism.\\nUpdate \\nVVConcat Concat \\nQ V\\nQ V\\nQs{KV}sCompressive memory & \\nLinear attention Causal scaled dot-product \\nattention & PE Linear \\nprojection \\n{KV}s-1Retrieve \\nFigure 1: Infini-attention has an addi-\\ntional compressive memory with linear\\nattention for processing infinitely long\\ncontexts. {KV}s−1and{KV}sare atten-\\ntion key and values for current and previ-\\nous input segments, respectively and Qs\\nthe attention queries. PE denotes position\\nembeddings.The attention mechanism in Transformers ex-\\nhibits quadratic complexity in both memory\\nfootprint and computation time. For example,\\nthe attention Key-Value (KV) states have 3TB\\nmemory footprint for a 500B model with batch\\nsize 512 and context length 2048 (Pope et al.,\\n2023). Indeed, scaling LLMs to longer sequences\\n(i.e. 1M tokens) is challenging with the standard\\nTransformer architectures and serving longer\\nand longer context models becomes costly finan-\\ncially.\\nCompressive memory systems promise to be\\nmore scalable and efficient than the attention\\nmechanism for extremely long sequences (Kan-\\nerva, 1988; Munkhdalai et al., 2019). Instead\\nof using an array that grows with the input se-\\nquence length, a compressive memory primarily\\nmaintains a fixed number of parameters to store\\nand recall information with a bounded storage\\nand computation costs. In the compressive mem-\\nory, new information is added to the memory\\nby changing its parameters with an objective\\nthat this information can be recovered back later\\non. However, the LLMs in their current state\\nhave yet to see an effective, practical compres-\\nsive memory technique that balances simplicity along with quality.\\n1arXiv:2404.07143v1  [cs.CL]  10 Apr 2024'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3cd57c-f82d-4469-bce1-5447bbb4ad02",
   "metadata": {},
   "source": [
    "## CHUNKING the PAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e716da2-df8f-4399-b0db-c8940adcb5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 568, which is longer than the specified 500\n",
      "Created a chunk of size 506, which is longer than the specified 500\n",
      "Created a chunk of size 633, which is longer than the specified 500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110\n",
      "<class 'langchain_core.documents.base.Document'>\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import NLTKTextSplitter\n",
    "\n",
    "text_splitter = NLTKTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "chunks = text_splitter.split_documents(pages)\n",
    "\n",
    "print(len(chunks))\n",
    "\n",
    "print(type(chunks[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab78a93-6d12-4c71-bd44-d917c4e249f1",
   "metadata": {},
   "source": [
    "## Passing the key and creating an Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6c76555-21bb-4e5e-83f0-f9bcc3552afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"C:\\csv\\key1.txt\")\n",
    "key=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c43868fd-9757-4de3-8839-f75c6ca5c6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embedding_model = GoogleGenerativeAIEmbeddings(google_api_key=key, \n",
    "                                               model=\"models/embedding-001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d89bcf-6979-4b38-b5f8-d1e3122bc935",
   "metadata": {},
   "source": [
    "## Storing the chunks in vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "421fd6b7-f071-4a31-9ab9-cc03ec965911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "db = Chroma.from_documents(chunks, embedding_model, persist_directory=\"./chroma_db_\")\n",
    "\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1b43bde3-b841-4e0c-9d17-d7799c4fe823",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_connection = Chroma(persist_directory=\"./chroma_db_\", embedding_function=embedding_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdab093c-21ca-4cc1-b17f-641e908750cf",
   "metadata": {},
   "source": [
    "## Setting-up vector store as a retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee3bc169-6a74-4b89-afdc-83e492874e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = db_connection.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2500e57-abf5-4662-a31d-6c62c8b474a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    # System Message Prompt Template\n",
    "    SystemMessage(content=\"\"\"You are a Helpful AI Bot. \n",
    "    You take the context and question from user. Your answer should be based on the specific context.\"\"\"),\n",
    "    # Human Message Prompt Template\n",
    "    HumanMessagePromptTemplate.from_template(\"\"\"Aswer the question based on the given context.\n",
    "    Context:\n",
    "    {context}\n",
    "    \n",
    "    Question: \n",
    "    {question}\n",
    "    \n",
    "    Answer: \"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cee40c2-a84d-40ba-9ec3-17243b6ac168",
   "metadata": {},
   "source": [
    "## LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "31e5594a-8346-4692-aca7-062720609a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "chat_model = ChatGoogleGenerativeAI(google_api_key=key, \n",
    "                                   model=\"gemini-1.5-pro-latest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a553f26b-3783-4623-822e-260a4f40c4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa714063-9666-4ea0-af29-def65cac19f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | chat_template\n",
    "    | chat_model\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1c0e16-56d1-45b1-bac5-09d02b660523",
   "metadata": {},
   "source": [
    "## Based on user query retrive the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7993f049-fb47-4abd-a447-9d3f898b22ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Leave No Context Behind: Understanding the Paper\\'s Core Idea\\n\\nBased on the provided context, the paper \"Leave No Context Behind\" appears to focus on introducing a novel attention mechanism for LLMs (Large Language Models) called **Infini-attention**. This mechanism aims to tackle the challenge of effectively handling both long and short-range contextual dependencies in text processing, a crucial aspect for accurate and nuanced language understanding.\\n\\nHere\\'s a breakdown of the paper\\'s key points:\\n\\n**Problem Addressed:**\\n\\n* Current LLMs lack efficient and practical methods for incorporating compressive memory, which is essential for capturing long-range context. \\n* Existing attention mechanisms struggle to balance simplicity with quality, often leading to complex implementations or compromised performance.\\n\\n**Proposed Solution: Infini-attention**\\n\\n* Combines local and global context states, similar to multi-head attention but with an added long-term memory component.\\n* This allows the model to effectively access and utilize information from both recent and distant parts of the input sequence.\\n*  Maintains a balance between practicality and performance by introducing minimal changes to the standard scaled dot-product attention.\\n\\n**Key Contributions:**\\n\\n1. **Powerful and Practical Attention:** Infini-attention offers an efficient solution for modeling long and short-range context, enhancing the LLM\\'s ability to understand complex relationships within text.\\n2. **Plug-and-Play Design:**  The mechanism seamlessly integrates with existing LLM architectures, supporting continual pre-training and adaptation to long-context scenarios.\\n\\n**Overall Significance:**\\n\\nThe paper presents a promising approach to improve LLM performance by effectively addressing the limitations of current attention mechanisms. By incorporating long-term memory and simplifying the attention process, Infini-attention has the potential to enhance various NLP tasks requiring a deep understanding of context, such as text summarization, question answering, and machine translation. \\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = rag_chain.invoke(\"What is this paper Leave No Context Behind all about\")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215a762d-afe6-466c-955a-bc714651da16",
   "metadata": {},
   "source": [
    "## Markdown the respone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7e083b6a-d904-40d4-ae37-c874b74e8247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Leave No Context Behind: Understanding the Paper's Core Idea\n",
       "\n",
       "Based on the provided context, the paper \"Leave No Context Behind\" appears to focus on introducing a novel attention mechanism for LLMs (Large Language Models) called **Infini-attention**. This mechanism aims to tackle the challenge of effectively handling both long and short-range contextual dependencies in text processing, a crucial aspect for accurate and nuanced language understanding.\n",
       "\n",
       "Here's a breakdown of the paper's key points:\n",
       "\n",
       "**Problem Addressed:**\n",
       "\n",
       "* Current LLMs lack efficient and practical methods for incorporating compressive memory, which is essential for capturing long-range context. \n",
       "* Existing attention mechanisms struggle to balance simplicity with quality, often leading to complex implementations or compromised performance.\n",
       "\n",
       "**Proposed Solution: Infini-attention**\n",
       "\n",
       "* Combines local and global context states, similar to multi-head attention but with an added long-term memory component.\n",
       "* This allows the model to effectively access and utilize information from both recent and distant parts of the input sequence.\n",
       "*  Maintains a balance between practicality and performance by introducing minimal changes to the standard scaled dot-product attention.\n",
       "\n",
       "**Key Contributions:**\n",
       "\n",
       "1. **Powerful and Practical Attention:** Infini-attention offers an efficient solution for modeling long and short-range context, enhancing the LLM's ability to understand complex relationships within text.\n",
       "2. **Plug-and-Play Design:**  The mechanism seamlessly integrates with existing LLM architectures, supporting continual pre-training and adaptation to long-context scenarios.\n",
       "\n",
       "**Overall Significance:**\n",
       "\n",
       "The paper presents a promising approach to improve LLM performance by effectively addressing the limitations of current attention mechanisms. By incorporating long-term memory and simplifying the attention process, Infini-attention has the potential to enhance various NLP tasks requiring a deep understanding of context, such as text summarization, question answering, and machine translation. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7509e66-7818-4f53-96f2-4989d8537ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
